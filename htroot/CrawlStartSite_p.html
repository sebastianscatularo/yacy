<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" >
<head>
  #%env/templates/metas.template%#
  <title>YaCy '#[clientname]#': Crawl Start</title>
  <script type="text/javascript" src="/js/yaml/pages/crawlStart.js"></script>
  <script type="text/javascript" src="/js/yaml/pages/crawlStartSite.js"></script>
</head>

<body id="crawlStartSite">
  <ul class="ym-skiplinks">
    <li><a class="ym-skip" href="#nav">Skip to navigation (Press Enter)</a></li>
    <li><a class="ym-skip" href="#main">Skip to main content (Press Enter)</a></li>
  </ul>

  #%env/templates/header.template%#

  <main>
    <div class="ym-wrapper">
      <div class="ym-wbox">
        <div id="api"></div>

<div id="devInfo">
Todo:
 - generate post URL
 - better help strings
 - check which hidden inputs are needed and which can be omitted as they are set to default values by Crawler_p
 - handle 'robots not allowed' when starting a crawl
 - handle form errors (missing fields)
Non working:
 - no form submission
</div>

        <h1>Site Crawling</h1>
        <p>
          <strong>Site Crawler:</strong>
          Download all web pages from a given domain or base URL.
        </p>

        <h2>Hints</h2>
        <ul>
          <li>
            <strong>Crawl Speed Limitation</strong><br/>
            No more that two pages are loaded from the same host in one second (not more that 120 document per minute) to limit the load on the target server.
          </li>
          <li>
            <strong>Target Balancer</strong><br/>
            A second crawl for a different host increases the throughput to a maximum of 240 documents per minute since the crawler balances the load over all hosts.
          </li>
          <li>
            <strong>High Speed Crawling</strong><br/>
            A 'shallow crawl' which is not limited to a single host (or site)
            can extend the pages per minute (ppm) rate to unlimited documents per minute when the number of target hosts is high.
            This can be done using the <a href="CrawlStartExpert_p.html">Expert Crawl Start</a> servlet.
          </li>
          <li>
            <strong>Scheduler Steering</strong><br/>
            The scheduler on crawls can be changed or removed using the <a href="Table_API_p.html">API Steering</a>.
          </li>
        </ul>

        <form method="post" action="Crawler_p.html" enctype="multipart/form-data" accept-charset="UTF-8">
          <fieldset class="main">
            <legend>Site Crawl Start</legend>

            <fieldset id="startPoint" class="formGroup">
              <legend>Site</legend>
              <fieldset class="formSection" data-id="startPoint">
                <legend>Crawl start point</legend>
                <div class="content">
                  <label for="crawlingURL" class="hidden">Crawl start point</label>
                  <input type="text" name="crawlingURL" id="crawlingURL">#[starturl]#</input>
                  <div class="formHint hidden">
                    <p class="ok">
                      The entered location looks correct.
                    </p>
                    <p class="error" data-id="invalid">
                      The entered URL is not valid.
                    </p>
                    <p class="error" data-id="empty">
                      You need to enter a URLs to start your crawl.
                    </p>
                    <p class="help">
                      Please define a start-url for your crawl.
                    </p>
                  </div>
                  <div data-id="getSiteData" class="hidden">
                    <button data-id="robotsAndStructure" class="icon-run">Check robots.txt &amp; get site structure</button>
                  </div>
                  <div id="startPointDetails" class="hidden">
                    <dl>
                      <dt data-id="bookmarkTitle">Bookmark title:</dt>
                      <dd data-id="bookmarkTitle"></dd>
                      <dt data-id="robotsAllowed">Robots allowed:</dt>
                      <dd data-id="robotsAllowed" class="listContent"></dd>
                      <dt data-id="linkList">Link-list:</dt>
                      <dd data-id="linkList" class="listContent"></dd>
                      <dt data-id="siteList">Site-list:</dt>
                      <dd data-id="siteList" class="listContent"></dd>
                    </dl>
                  </div>
                </div>
                <div class="formHelp hidden">
                  <p>
                    This URL is the root for a crawl start, existing start URLs are always re-loaded. Already visited URLs are being re-crawled.
                  </p>
                  <p>
                    URLs must start with <em>http://…</em>, <em>https://…</em>, <em>ftp://…</em>, <em>smb://…</em> or <em>file://…</em>.
                  </p>
                </div>
              </fieldset>
              <fieldset id="startPointSelect" class="formSection hidden">
                <legend>Start point selection</legend>
                <div class="content">
                  <label for="startPointSelectBox">Start crawl from</label>
                  <select id="startPointSelectBox" data-review="custom">
                    <option data-id="urlList" selected="selected">URL-list</option>
                    <option data-id="linkList">Link-list</option>
                    <option data-id="siteList">Site-list</option>
                  </select>
                </div>
              </fieldset>
            </fieldset>

            <fieldset class="formGroup">
              <legend>Path</legend>
              <fieldset class="formSection control-group" data-review="skip-title">
                <legend>Full domain</legend>
                <div class="content">
                  <input type="radio" name="range" id="rangeDomain" value="domain" checked="checked"/>
                  <label for="rangeDomain">load all files in domain</label>
                </div>
              </fieldset>
              <fieldset class="formSection control-group" data-review="skip-title">
                <legend>Sub-path</legend>
                <div class="content">
                  <input type="radio" name="range" id="rangeSubpath" value="subpath" />
                  <label for="rangeSubpath">load only files in a sub-path of given url</label>
                </div>
              </fieldset>
            </fieldset>

            <fieldset class="formGroup">
              <legend>Limitation</legend>
              <fieldset class="formSection toggleable" data-toggle-type="activate" data-toggle-fieldid="maxPagesPerDomain">
                <legend>Maximum Pages per Domain</legend>
                <div class="content">
                  <div class="control-group">
                    <label for="crawlingDomMaxPages">Set the maximum number of loaded pages to</label>
                    <input name="crawlingDomMaxPages" id="crawlingDomMaxPages" type="text" size="6" maxlength="6" value="#[crawlingDomMaxPages]#" class="spinner"/>.
                  </div>
                </div>
                <div class="formHelp hidden">
                  <p>
                    You can limit the maximum number of pages that are fetched and indexed from a single domain with this option.
                  </p>
                </div>
              </fieldset>
            </fieldset>

            <fieldset class="formGroup">
              <legend>Collection(s)</legend>
              <fieldset class="formSection toggleable">
                <legend>Result collection(s)</legend>
                <div class="content">
                  <div class="control-group">
                    <label for="collection">Target collection(s)</label>
                    <input name="collection" id="collection" type="text" size="60" maxlength="100" value="#[collection]#" #(collectionEnabled)#disabled="disabled"::#(/collectionEnabled)# placeholder="#[defaultCollection]#"/>
                    <div class="formHint hidden">
                      <p class="help">
                        Please enter a comma seperated list of collections, where the results should be added.
                      </p>
                    </div>
                  </div>
                </div>
                <div class="formHelp hidden">
                  <p>
                    A crawl result can be tagged with names which are candidates for a collection request.
                  </p>
                  <p>
                    These tags can be selected with the <a href="/gsa/search?q=www&site=#[collection]#">GSA interface</a> using the 'site' operator.
                    To use this option, the 'collection_sxt'-field must be switched on in the <a href="/IndexFederated_p.html">Solr Schema</a>
                  </p>
                </div>
              </fieldset>
            </fieldset>

            <input type="hidden" name="crawlingDepth" id="crawlingDepth" value="99" />
            <input type="hidden" name="deleteold" id="deleteold" value="on" />
            <input type="hidden" name="mustnotmatch" id="mustnotmatch" value="" />
            <input type="hidden" name="crawlingDomFilterCheck" id="crawlingDomFilterCheck" value="off" />
            <input type="hidden" name="crawlingDomFilterDepth" id="crawlingDomFilterDepth" value="#[crawlingDomFilterDepth]#" />
            <input type="hidden" name="directDocByURL" id="directDocByURL" value="off" />
            <input type="hidden" name="recrawl" id="recrawl" value="reload" />
            <input type="hidden" name="reloadIfOlderNumber" id="reloadIfOlderNumber" value="3" />
            <input type="hidden" name="reloadIfOlderUnit" id="reloadIfOlderUnit" value="day" />
            <input type="hidden" name="deleteold" id="deleteold" value="on" />
            <input type="hidden" name="storeHTCache" id="storeHTCache" value="on" />
            <input type="hidden" name="cachePolicy" id="cachePolicy" value="iffresh" />
            <input type="hidden" name="crawlingQ" id="crawlingQ" value="on" />
            <input type="hidden" name="followFrames" id="followFrames" value="on" />
            <input type="hidden" name="obeyHtmlRobotsNoindex" id="obeyHtmlRobotsNoindex" value="on" />
            <input type="hidden" name="indexText" id="indexText" value="on" />
            <input type="hidden" name="indexMedia" id="indexMedia" value="on" />
            <input type="hidden" name="intention" id="intention" value="" />

            <fieldset class="formControl" data-id="formControl">
              <div class="content">
                <button type="submit" class="primary icon-run">Start crawl job <span class="errors"></span></button>
              </div>
            </fieldset>
          </fieldset>
        </form>

        #%env/templates/footer.template%#
      </div>
    </div>
  </main>
</body>
</html>
